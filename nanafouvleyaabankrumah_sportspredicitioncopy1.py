# -*- coding: utf-8 -*-
"""NanaFouvleYaabaNkrumah_SportsPredicitioncopy1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yBxnU2JTJ_frp_4SJpqC0sKrY2kwQ9Bw
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

"""## Loading the data files and cleaning them"""

from google.colab import drive
drive.mount('/content/drive')

# Load the datasets
male_players_training = pd.read_csv('/content/drive/MyDrive/male_players (legacy).csv')
players_22_testing= pd.read_csv('/content/drive/MyDrive/players_22.csv')

male_players_training

male_players_training.columns

male_players_training= male_players_training.drop(['player_face_url' , 'player_url','fifa_version', 'fifa_update','fifa_update_date','wage_eur'  ] , axis = 1)

#turning id into int
male_players_training['player_id'] = male_players_training['player_id'].astype(int)

male_players_training.columns

#dropping the date of birth
male_players_training= male_players_training.drop(['dob','player_id', 'short_name', 'long_name', 'player_positions']  , axis = 1)

male_players_training.columns

"""##### Reasons For Dropping Columns

I dropped the following columns because when I read the descriptions of what they meant on Kaggle. I realised that they will not affect the prediction of a player's overall rating given a profile of the player
"""

male_players_training.drop(['league_id', 'league_name', 'league_level' , 'league_id' , 'club_team_id', 'club_name', 'club_position', 'club_jersey_number', 'club_loaned_from', 'club_joined_date', 'club_contract_valid_until_year', 'nationality_id', 'nationality_name',
                            'nation_team_id', 'nation_position', 'nation_jersey_number', 'release_clause_eur', 'club_contract_valid_until_year','nationality_id',
        'nation_team_id', 'nation_position', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
    'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
    'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'
  ], axis= 1, inplace=True)

male_players_training.columns

male_players_training

male_players_training.info() #to check if there are missing values

#dropping the columns that have 30% missing values
L = []
L_less = []
for i in male_players_training.columns:
    if(male_players_training[i].isnull().sum()<(0.3*male_players_training.shape[0])):
        L.append(i)
    else:
        L_less.append(i)

#separating numeric and non-numeric
numeric_data = male_players_training.select_dtypes(include= np.number)
non_numeric = male_players_training.select_dtypes(exclude= np.number)

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp = IterativeImputer(max_iter=20, tol=1e-1, random_state=0)

# Fit and transform the data
numeric_data_imputed = imp.fit_transform(numeric_data)

# Round the imputed data and convert it back to a DataFrame
numeric_data = pd.DataFrame(np.round(numeric_data_imputed), columns=numeric_data.columns)

numeric_data

numeric_data.head()

numeric_data.iloc[30:50,:]

numeric_data.info()

non_numeric

#dropping the columns that have 30% missing values
n = []
n_less = []
for i in non_numeric.columns:
    if(non_numeric[i].isnull().sum()<(0.3*non_numeric.shape[0])):
        n.append(i)
    else:
        n_less.append(i)
non_numeric= non_numeric[n]

#filling the NaNs in the columns with the forward fill method
non_numeric.ffill()

non_numeric.info()

from sklearn.preprocessing import LabelEncoder, StandardScaler

#encoding the columns
label = LabelEncoder()

#creating a for loop to encode each column
for i in non_numeric.columns:
    non_numeric[i] = label.fit_transform(non_numeric[i])

non_numeric

#putting the numeric and non_numeric data together
df = pd.concat((numeric_data, non_numeric), axis = 1)
df

print(np.isnan(df).sum())  # Should output 0 if there are no NaN values

original_y = df['overall']

df.drop('overall', axis=1 , inplace=True)
original_x = df

#scaling the original matrix
scaler = StandardScaler()
original_X =  scaler.fit_transform(original_x)

Xtrain,Xtest,Ytrain,Ytest = train_test_split(original_X, original_y,test_size=0.2,random_state=42)

Xtrain = pd.DataFrame(Xtrain, columns=df.columns)

from sklearn.ensemble import RandomForestRegressor

# Initialize and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(Xtrain, Ytrain)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({
    'Feature': Xtrain.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

feature_importance_df

"""#### Create a subset of important features"""

# Set the number of top features to select
k = 10

# Select the top-k features
top_k_features = feature_importance_df.head(k)['Feature']


#The important features
print(top_k_features)

#selecting the top features from the original dataframe
selected_features = df[top_k_features]
selected_features

"""#### Creating and training Ensemble Models"""

import pickle
filename = 'scaler.pkl'
pickle.dump(scaler, open(filename, 'wb'))

X = selected_features

Xtrain,Xtest,Ytrain,Ytest = train_test_split(X, original_y,test_size=0.2,random_state=42)

# MODEL  1      -  RANDOM   FOREST   REGRESSOR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the Random Forest Regressor model
rf_model = RandomForestRegressor(random_state=42)

# Train the model on the training data
rf_model.fit(Xtrain, Ytrain)

# Predictions on the validation set
predictions = rf_model.predict(Xtest)

# Evaluate the model
mae = mean_absolute_error(Ytest, predictions)
rmse = np.sqrt(mean_squared_error(Ytest, predictions))
r2 = r2_score(Ytest, predictions)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

# MODEL  2      -     GRADIENT   BOOSTING   REGRESSOR
from sklearn.ensemble import GradientBoostingRegressor


# Initialize the model
gb_model = GradientBoostingRegressor(random_state=42)


# Train the model on the training data
gb_model.fit(Xtrain, Ytrain)


# Predictions on the validation set
predictions = gb_model.predict(Xtest)

# Evaluate the model
mae = mean_absolute_error(Ytest, predictions)
rmse = np.sqrt(mean_squared_error(Ytest, predictions))
r2 = r2_score(Ytest, predictions)


print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

# MODEL  3 - XG   BOOST   REGRESSOR

from xgboost import XGBRegressor

# Initialize the model
xgb_model = XGBRegressor(random_state=42)

# Train the model on the training data
xgb_model.fit(Xtrain, Ytrain)


# Predictions on the validation set
predictions = xgb_model.predict(Xtest)

# Evaluate the model
mae = mean_absolute_error(Ytest, predictions)
rmse = np.sqrt(mean_squared_error(Ytest, predictions))
r2 = r2_score(Ytest, predictions)


print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

# MODEL  4 - ADA   BOOST  REGRESSOR
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the AdaBoost Regressor model
adb_model = AdaBoostRegressor(random_state=42)

# Train the model on the training data
adb_model.fit(Xtrain, Ytrain)

# Predictions on the validation set
predictions = adb_model.predict(Xtest)

# Evaluate the model
mae = mean_absolute_error(Ytest, predictions)
rmse = np.sqrt(mean_squared_error(Ytest, predictions))
r2 = r2_score(Ytest, predictions)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

"""#### Cross-Validation to get the Best Ensemble model"""

from sklearn.model_selection import GridSearchCV, KFold

# Define parameters for grid search
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

param_grid_gb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.1, 0.05]
}

param_grid_xgb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.1, 0.05],
    'max_depth': [3, 5, 7]
}

param_grid_ada = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [1.0, 0.5, 0.1]
}

# Initialize KFold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV for Random Forest
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_rf.fit(Xtrain, Ytrain)

# Initialize GridSearchCV for Gradient Boosting
grid_search_gb = GridSearchCV(estimator=gb_model, param_grid=param_grid_gb, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_gb.fit(Xtrain, Ytrain)

# Initialize GridSearchCV for XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_xgb.fit(Xtrain, Ytrain)

# Initialize GridSearchCV for AdaBoost
grid_search_ada = GridSearchCV(estimator=adb_model, param_grid=param_grid_ada, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_ada.fit(Xtrain, Ytrain)

# Get best parameters for Random Forest
best_params_rf = grid_search_rf.best_params_

# Train Random Forest with best parameters
best_rf_model = RandomForestRegressor(**best_params_rf, random_state=42)
best_rf_model.fit(Xtrain, Ytrain)

# Predict and evaluate on validation set
y_pred_rf = best_rf_model.predict(Xtest)
mae_rf = mean_absolute_error(Ytest, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(Ytest, y_pred_rf))
r2_rf = r2_score(Ytest, y_pred_rf)

print("Random Forest Regressor")
print(f"Mean Absolute Error: {mae_rf:.2f}")
print(f"Root Mean Squared Error: {rmse_rf:.2f}")
print(f"R-squared: {r2_rf:.2f}")

# Get best parameters for Gradient Boosting
best_params_gb = grid_search_gb.best_params_

# Train Gradient Boosting with best parameters
best_gb_model = GradientBoostingRegressor(**best_params_gb, random_state=42)
best_gb_model.fit(Xtrain, Ytrain)

# Predict and evaluate on validation set
y_pred_gb = best_gb_model.predict(Xtest)
mae_gb = mean_absolute_error(Ytest, y_pred_gb)
rmse_gb = np.sqrt(mean_squared_error(Ytest, y_pred_gb))
r2_gb = r2_score(Ytest, y_pred_gb)

print("Gradient Boosting Regressor")
print(f"Mean Absolute Error: {mae_gb:.2f}")
print(f"Root Mean Squared Error: {rmse_gb:.2f}")
print(f"R-squared: {r2_gb:.2f}")

# Get best parameters for XGBoost
best_params_xgb = grid_search_xgb.best_params_

# Train XGBoost with best parameters
best_xgb_model = XGBRegressor(**best_params_xgb, random_state=42)
best_xgb_model.fit(Xtrain, Ytrain)

# Predict and evaluate on the test set
y_pred_xgb = best_xgb_model.predict(Xtest)
mae_xgb = mean_absolute_error(Ytest, y_pred_xgb)
rmse_xgb= np.sqrt(mean_squared_error(Ytest, y_pred_xgb))
r2_xgb = r2_score(Ytest, y_pred_xgb)

print("XGB Boost Regressor")
print(f"Mean Absolute Error: {mae_xgb:.2f}")
print(f"Root Mean Squared Error: {rmse_xgb:.2f}")
print(f"R-squared: {r2_xgb:.2f}")


# Get best parameters for AdaBoost
best_params_ada = grid_search_ada.best_params_

# Train AdaBoost with best parameters
best_ada_model = AdaBoostRegressor(**best_params_ada, random_state=42)
best_ada_model.fit(Xtrain, Ytrain)

# Predict and evaluate on the test set
y_pred_ada = best_ada_model.predict(Xtest)
mae_ada = mean_absolute_error(Ytest, y_pred_ada)
rmse_ada = np.sqrt(mean_squared_error(Ytest, y_pred_ada))
r2_ada = r2_score(Ytest, y_pred_ada)

print("Ada Boost Regressor")
print(f"Mean Absolute Error: {mae_ada:.2f}")
print(f"Root Mean Squared Error: {rmse_ada:.2f}")
print(f"R-squared: {r2_ada:.2f}")

# Compare XGBoost and AdaBoost
if rmse_xgb < rmse_ada:
    best_ensemble_model = best_xgb_model
    best_ensemble_params = best_params_xgb
    best_ensemble_rmse = rmse_xgb
else:
    best_ensemble_model = best_ada_model
    best_ensemble_params = best_params_ada
    best_ensemble_rmse = rmse_ada

# Compare Random Forest and Gradient Boosting
if rmse_rf < rmse_gb:
    best_forest_model = best_rf_model
    best_forest_params = best_params_rf
    best_forest_rmse = rmse_rf
else:
    best_forest_model = best_gb_model
    best_forest_params = best_params_gb
    best_forest_rmse = rmse_gb

# Compare the best models from each category
if best_ensemble_rmse < best_forest_rmse:
    best_model = best_ensemble_model
    best_params = best_ensemble_params
    best_mse = best_ensemble_rmse
else:
    best_model = best_forest_model
    best_params = best_forest_params
    best_mse = best_forest_rmse


# Evaluate on test set
y_pred_test = best_model.predict(Xtest)
mse_test = mean_squared_error(Ytest, y_pred_test)

print("Best Model:", best_model)
print("Best Hyperparameters:", best_params)
print("MSE on Test Set:", mse_test)

y_pred_test

#Saving Model as File
import pickle
filename = 'model.pkl'
pickle.dump(rf_model, open(filename , 'wb'))

"""## Using the data from Players_22 data"""

players_22_testing.columns

"""### Data Cleaning"""

selected_columns_test = ['overall','potential','value_eur','wage_eur','dribbling','movement_reactions','mentality_composure','international_reputation','skill_ball_control','goalkeeping_diving','goalkeeping_positioning']
test_columns = players_22_testing[selected_columns_test]
test_columns

test_columns.columns

#creating a function to do data cleaning
def preprocess_data(df):
    """
    Preprocesses the data by performing the following steps:
    1. Drops columns with more than 30% missing values.
    2. Imputes missing values in numeric columns using IterativeImputer.
    3. Forward fills missing values in non-numeric columns.
    4. Encodes non-numeric columns using LabelEncoder.
    5. Concatenates the processed numeric and non-numeric data back together.

    Parameters:
    df (pd.DataFrame): The input DataFrame to preprocess.

    Returns:
    pd.DataFrame: The preprocessed DataFrame.
    """

    # Dropping columns that have more than 30% missing values
    columns_to_keep = []
    columns_to_drop = []
    for col in df.columns:
        if df[col].isnull().sum() < (0.3 * df.shape[0]):
            columns_to_keep.append(col)
        else:
            columns_to_drop.append(col)

    df = df[columns_to_keep]

    # Separating numeric and non-numeric data
    numeric_data = df.select_dtypes(include=np.number)
    non_numeric_data = df.select_dtypes(exclude=np.number)

    # Imputing missing values in numeric data
    imp = IterativeImputer(max_iter=10, tol=1e-1, random_state=0)
    numeric_data_imputed = imp.fit_transform(numeric_data)
    numeric_data = pd.DataFrame(np.round(numeric_data_imputed), columns=numeric_data.columns)

    # Dropping columns in non-numeric data that have more than 30% missing values
    non_numeric_data = non_numeric_data.loc[:, non_numeric_data.isnull().mean() < 0.3]

    # Filling NaNs in non-numeric data with forward fill method
    non_numeric_data.ffill(inplace=True)

    # Encoding non-numeric columns
    label_encoder = LabelEncoder()
    for col in non_numeric_data.columns:
        non_numeric_data[col] = label_encoder.fit_transform(non_numeric_data[col])

    # Concatenating the numeric and non-numeric data back together
    df_processed = pd.concat([numeric_data, non_numeric_data], axis=1)

    return df_processed

# Usage:
df_processed = preprocess_data(test_columns)

"""### Model Training"""

#creating our y and X
yt = df_processed['overall']
df_processed.drop('overall', axis=1 , inplace=True)
Xt = df_processed

Xt.columns

#scaling the data down
scaled_feature_test = scaler.fit_transform(Xt)
scaled_test = pd.DataFrame(Xt, columns=Xt.columns)

#Train Split
Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.2, random_state=42)
rf_model.fit(Xt_train, yt_train)

# Predictions on the validation set
predictions_22 = rf_model.predict(Xt_test)

# Evaluate the model
mae = mean_absolute_error(yt_test, predictions_22)
rmse = np.sqrt(mean_squared_error(yt_test, predictions_22))
r2 = r2_score(yt_test, predictions_22)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

values = {"Ytest": yt_test, "Ypred": predictions_22}
df = pd.DataFrame(values)

df.to_csv('ytest-ypred.csv', index=False)